{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 4: Нейронный машинный перевод\n",
    "\n",
    "Набор данных: http://www.manythings.org/anki (как часть проекта Tatoeba)\n",
    "\n",
    "Целью лабораторной работы является построение end-to-end прототипа системы нейронного машинного перевода.\n",
    "\n",
    "Справочный материал: https://machinelearningmastery.com/\n",
    "\n",
    "**Опишите Вашу языковую пару**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка текста\n",
    "\n",
    "Задание: загрузить файл.\n",
    "Необходимо\n",
    "* произвести загрузку файла\n",
    "* очистку текста (удалить пунктуацию, привести к нижнему регистру и т.д.)\n",
    "* Разделить файл по парам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_document(filename):\n",
    "    f = open(filename, mode='rt', encoding='utf-8')\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждая строка содержит языковую пару, разделенную символом табуляции, необходимо разделить их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in lines]\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведите процедуру \"очистки текста\".\n",
    "Необходимо: \n",
    "* Удалить пунктуацию\n",
    "* Удалить непечатные символы\n",
    "* Выполнить нормализацию регистра\n",
    "* удалить ненужные токены\n",
    "и т.д.\n",
    "\n",
    "**Задание:** напишите функцию очистки языковых пар"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Не совсем понимаю, что подразумевается по ненужными словами. Если речь идет о стоп словах, то я считаю для NMT, удалять их не нужно "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_pairs(lines):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку процесс предварительной обработки файлов небыстрый, может быть полезно после очистки сохранить структуру данных, в которой содержатся обработанные языковые пары. \n",
    "Сохраните эту структуру для того, чтобы не ждать подолгу очистки и подготовки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "def save_clean_data(semtences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('File was saved to {}'.format(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После написания необходимых подготовительных функций, запустим:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "target_filename = ...\n",
    "# Загрузка датасета\n",
    "doc = load_document(target_filename)\n",
    "\n",
    "pairs = transform_to_pairs(doc)\n",
    "# Очистка предложений\n",
    "clean_pairs = clean_pairs(pairs)\n",
    "# Сохранение обработанных данных\n",
    "# Выберете более адекватное имя файла\n",
    "save_clean_data(clean_pairs, 'sentence-pairs.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Выведите количество пар предложений в Вашем корпусе?\n",
    "* Определите число словоформ в корпусе (для обоих языков);\n",
    "* Какая максимальная длина предложения в корпусе (для обоих языков)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Файл, с языковыми парами содержит большое число переводных пар. Однако, наши ресурсы не позволяют загрузить весь набор. Поэтому выберем только выберем только часть из них.\n",
    "Задание: \n",
    "* Загрузите сохраненный файл\n",
    "* Выберете число примеров, на которых вы будете проводить обучение/тестирование (не менее 10 000).\n",
    "* Перемешайте выборку\n",
    "* Разделите на тестовую и тренировочную выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Открываем сохраненный файл\n",
    "def load_clean_sentences(fname):\n",
    "    return load(open(fname, 'rb'))\n",
    "\n",
    "def save_clean_data(sentences, fname):\n",
    "    dump(sentences, open(fname, 'wb'))\n",
    "    print('Saved: {}'.format(fname))\n",
    "\n",
    "raw_dataset = load_clean_sentences('sentence-pairs.pkl')\n",
    "\n",
    "N_SENTENCES = ...\n",
    "\n",
    "shuffle(raw_dataset)\n",
    "dataset = raw_dataset[:N_SENTENCES, :]\n",
    "\n",
    "train, test = ...\n",
    "\n",
    "#\n",
    "save_clean_data(dataset, 'subset.pkl')\n",
    "save_clean_data(train, ...)\n",
    "save_clean_data(test, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вышеперечисленный код создаст три файла:\n",
    "* вся подвыборка\n",
    "* тренировочная выборка\n",
    "* тестовая выборка\n",
    "\n",
    "Это нужно для стабильности последующих экспериментов.\n",
    "* Какой объем словаря в выбранном Вами корпусе?\n",
    "* Выведете процентное соотношение объема в вашей подвыборке относительно всего словаря\n",
    "\n",
    "\n",
    "## Обучение модели нейронного машинного перевода\n",
    "\n",
    "В этой части лабораторной работы произведем загрузку и подготовку уже предобработанного текста, а затем подготовку токенизатора, создание нейронной модели, и непосредственно обучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# load datasets\n",
    "dataset = load_clean_sentences(...)\n",
    "train = load_clean_sentences(...)\n",
    "test = load_clean_sentences(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Упростим себе задачу небольшой хитростью: будем использовать комбинацию тренировочной и тестовой выборки для того, чтобы узнать весь словарь в задаче, и определить максимальную длину предложения.\n",
    "\n",
    "Разумеется, это достаточно сильное упрощение, но для целей обучения его можно использовать.\n",
    "Создадим токенизатор:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# Функция для получения максимальной длины предложений\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно вызывать эти функции для токенизации.\n",
    "\n",
    "Обратите внимание, нужно построить два токенизатора: для исходного и для целевого языка:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Лучше имя токенизатора переименовать к вашему языку:\n",
    "first_tokenizer = create_tokenizer(dataset[:,0])\n",
    "first_language_vocab_size = len(first_tokenizer)+1\n",
    "first_language_length = max_length(dataset[:,0])\n",
    "print('First language vocabulary size: {}'.format(first_language_vocab_size))\n",
    "print('First language max length of sentence: {}'.format(first_language_length))\n",
    "\n",
    "# Сделаейте аналогичное для второго языка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как Вы помните, текст нельзя передавать напрямую нейронной сети. Каждая входная и выходная последовательности должны быть закодированы целыми числами и дополнены до максимальной длины (padding).\n",
    "\n",
    "Это необходимо для использования embedding;ов для входных последовательностей и унитарное кодирование для выходных последовательностей.\n",
    "За счет этого необходимо разработать две функции:\n",
    "* encode_sequences - которая будет кодировать текст как целочисленные значения и дополнять до максимальной длины\n",
    "* encode_output - которая будет проводить унитарное кодирование для последовательностей целевого языка. Унитарное кодирование неоходиомо за счет того что мы будем предсказывать вероятность каждого слова в словаре (на выходе)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "        y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Время подготовить тестовую и тренировочную выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX = encode_sequences(first_tokenizer, first_language_length, ...)\n",
    "trainY = encode_sequences(second_tokenizer, second_anguage_length, ...)\n",
    "trainY = encode_output(trainY, second_language_vocabulary_size)\n",
    "\n",
    "# Сделайте для testX, testY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Время построить модель NMT системы.\n",
    "\n",
    "Постройте модель encoder-decoder на базе LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_model(src_vocab, target_vocab,\n",
    "                src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(...))\n",
    "    # ...\n",
    "    # ...\n",
    "    # PROFIT!\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    model.compile(optimizer='adam', loss='categorial_crossentropy')\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Время обучить модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', verbose=1,\n",
    "save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, \n",
    "          epochs=30, \n",
    "          batch_size=64, \n",
    "          validation_data=(testX, testY),\n",
    "callbacks=[checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте модель.\n",
    "При необходимости загрузите сохраненную модель\n",
    "\n",
    "        model = load_model('model.h5')\n",
    "        \n",
    "Если загружаете произвольный текст не забудьте проводить кодирование при помощи\n",
    "        \n",
    "        encode_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценка включает в себя два шага:\n",
    "* Генерация переведенной выходной последовательности\n",
    "* Повторение процесса для ряда примеров и суммаризация способности модели переводить предложения.\n",
    "    \n",
    "Пример перевода:\n",
    "        \n",
    "        translation = model.predict(source, verbose=0)\n",
    "\n",
    "Не забывайте(!) что мы должны обратно преобразовать поседовательность чисел в фактические слова. Результатом predict будет последовательность целых чисел, которую мы преобразовываем обратно в слова.\n",
    "Функция word_for_id выполняет обратное преобразование:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно выполнить отображение каждого целого числа в переводе и вернуть результат как строку слов.\n",
    "Функция predict_sequence() выполняет эту операцию для ОДНОЙ закодированной фразы:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "    target.append(word)\n",
    "    return ' '.join(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Время оценить модель: **напишите функцию**:\n",
    "        \n",
    "    evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "\n",
    "Которая на вход принимает модель, токенизатор, исходные тексты и эталонный результат.\n",
    "\n",
    "Подсчитайте BLEU-1, BLEU-2, BLEU-3, BLEU-4 для тестовой выборки, для тренировочной выборки\n",
    "e\n",
    "\n",
    "BLEU входит в состав NLTK: \n",
    "\n",
    "    from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "Подсчет BLEU:\n",
    "\n",
    "    BLEU-1: corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))\n",
    "    BLEU-2: corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))\n",
    "    BLEU-3: corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0))\n",
    "    BLEU-4: corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**:\n",
    "Для Ваших языков примените построенную модель с указанными параметрами.\n",
    "Запишите результаты в таблицу:\n",
    "\n",
    "| Параметры  | BLEU-1  | BLEU-2 | BLEU-3 | BLEU-4 |\n",
    "|:---------- |:-------:|:------:|:------:|:------:|\n",
    "| Embedding=32  |      |        |        |        |\n",
    "| Embedding=64  |      |        |        |        |\n",
    "| Embedding=128  |      |        |        |        |\n",
    "| Embedding=256  |      |        |        |        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Попробуйте добавить: (Задание для вашего варианта)\n",
    "Как изменятся результаты? Чем это может быть обусловлено?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
